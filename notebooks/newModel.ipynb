{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Domain               100 non-null    object \n",
      " 1   Visits               100 non-null    float64\n",
      " 2   Desktop Share        100 non-null    float64\n",
      " 3   Mobile Share         100 non-null    float64\n",
      " 4   MoM                  98 non-null     float64\n",
      " 5   YoY                  94 non-null     float64\n",
      " 6   Main Traffic Source  100 non-null    object \n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 5.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/DataSet_WebPhishing_Final.csv')\n",
    "df_urls_legit = pd.read_csv('../datasets/top_websites.csv', delimiter=';')\n",
    "print(df_urls_legit.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_levenshtein_distance(url, legit_urls):\n",
    "    distances = [Levenshtein.distance(url, legit_url) for legit_url in legit_urls]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variantes de phishing generadas:\n",
      "['y0utube.c0m', 'youtub3.com', 'youtube.comi', 'youtube.co', 'faceb00k.c0m', 'fac3book.com', 'f4cebook.com', 'facebook.comh', 'facebook.co', 'g00gle.c0m', 'googl3.com', 'google.comn', 'google.co']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generar_variaciones(url):\n",
    "    variaciones = []\n",
    "    # Reemplazo de caracteres comunes\n",
    "    reemplazos = {'o': '0', 'i': '1', 'e': '3', 'a': '4', 's': '5'}\n",
    "    \n",
    "    # Generar una variación reemplazando letras\n",
    "    for original, reemplazo in reemplazos.items():\n",
    "        if original in url:\n",
    "            variaciones.append(url.replace(original, reemplazo))\n",
    "    \n",
    "    # Agregar o eliminar letras\n",
    "    variaciones.append(url + random.choice(\"abcdefghijklmnopqrstuvwxyz\"))\n",
    "    variaciones.append(url[:-1])  # Eliminar el último caracter\n",
    "    \n",
    "    return variaciones\n",
    "\n",
    "# Ejemplo con URLs legítimas\n",
    "legit_urls = [\"youtube.com\", \"facebook.com\", \"google.com\"]\n",
    "\n",
    "phishing_urls = []\n",
    "\n",
    "for url in legit_urls:\n",
    "    phishing_urls.extend(generar_variaciones(url))\n",
    "\n",
    "print(\"Variantes de phishing generadas:\")\n",
    "print(phishing_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a list of legitimate URLs. Generate phishing-like variations for each URL that look visually similar but contain small differences commonly used in phishing attacks. Make sure each variation could be mistaken for the original.\n",
      "\n",
      "- google.com\n",
      "\n",
      "- youtube.com\n",
      "\n",
      "- facebook.com\n",
      "\n",
      "- pornhub.com\n",
      "\n",
      "- xvideos.com\n",
      "\n",
      "- twitter.com\n",
      "\n",
      "- wikipedia.org\n",
      "\n",
      "- reddit.com\n",
      "\n",
      "- instagram.com\n",
      "\n",
      "- asura.gg\n",
      "\n",
      "- chapmanganato.com\n",
      "\n",
      "- xnxx.com\n",
      "\n",
      "- lectortmo.com\n",
      "\n",
      "- yahoo.com\n",
      "\n",
      "- spankbang.com\n",
      "\n",
      "- amazon.com\n",
      "\n",
      "- xhamster.com\n",
      "\n",
      "- weather.com\n",
      "\n",
      "- fandom.com\n",
      "\n",
      "- yandex.ru\n",
      "\n",
      "- nhentai.net\n",
      "\n",
      "- tiktok.com\n",
      "\n",
      "- exdynsrv.com\n",
      "\n",
      "- yahoo.com.jp\n",
      "\n",
      "- manganato.com\n",
      "\n",
      "- duckduckgo.com\n",
      "\n",
      "- xhamster18.desi\n",
      "\n",
      "- animeflv.net\n",
      "\n",
      "- search-hub.com\n",
      "\n",
      "- livedoor.jp\n",
      "\n",
      "- twitch.tv\n",
      "\n",
      "- blogspot.com\n",
      "\n",
      "- bing.com\n",
      "\n",
      "- dood.re\n",
      "\n",
      "- archiveofourown.org\n",
      "\n",
      "- rule34.xxx\n",
      "\n",
      "- whatsapp.com\n",
      "\n",
      "- live.com\n",
      "\n",
      "- bit.ly\n",
      "\n",
      "- vk.com\n",
      "\n",
      "- mangabuddy.com\n",
      "\n",
      "- youtu.be\n",
      "\n",
      "- linkedin.com\n",
      "\n",
      "- quora.com\n",
      "\n",
      "- adsmoloco.com\n",
      "\n",
      "- pixiv.net\n",
      "\n",
      "- chaturbate.com\n",
      "\n",
      "- microsoft.com\n",
      "\n",
      "- netflix.com\n",
      "\n",
      "- hitoi.la\n",
      "\n",
      "- t.me\n",
      "\n",
      "- blog.jp\n",
      "\n",
      "- voiranime.com\n",
      "\n",
      "- office.com\n",
      "\n",
      "- naver.com\n",
      "\n",
      "- microsoftonline.com\n",
      "\n",
      "- asurascans.com\n",
      "\n",
      "- imdb.com\n",
      "\n",
      "- onlyfans.com\n",
      "\n",
      "- chapmanganelo.com\n",
      "\n",
      "- komikcast.site\n",
      "\n",
      "- ts-tracker.me\n",
      "\n",
      "- cnn.com\n",
      "\n",
      "- googlevideo.com\n",
      "\n",
      "- onelink.me\n",
      "\n",
      "- walmart.com\n",
      "\n",
      "- gogoanime.ar\n",
      "\n",
      "- reaperscans.com\n",
      "\n",
      "- paypal.com\n",
      "\n",
      "- toonily.com\n",
      "\n",
      "- hanie.tv\n",
      "\n",
      "- aliexpress.com\n",
      "\n",
      "- lectormanga.com\n",
      "\n",
      "- marca.com\n",
      "\n",
      "- mediafire.com\n",
      "\n",
      "- mangago.me\n",
      "\n",
      "- livedoor.com\n",
      "\n",
      "- webtoon.xyz\n",
      "\n",
      "- animixplay.to\n",
      "\n",
      "- bbc.com\n",
      "\n",
      "- pinterest.com\n",
      "\n",
      "- zoo.us\n",
      "\n",
      "- mega.nz\n",
      "\n",
      "- nytimes.com\n",
      "\n",
      "- app.link\n",
      "\n",
      "- mail.ru\n",
      "\n",
      "- ebay.com\n",
      "\n",
      "- twimg.com\n",
      "\n",
      "- discord.com\n",
      "\n",
      "- github.com\n",
      "\n",
      "- nightskyclub.com\n",
      "\n",
      "- bbc.co.uk\n",
      "\n",
      "- spotify.com\n",
      "\n",
      "- apple.com\n",
      "\n",
      "- sxyprn.com\n",
      "\n",
      "- 1stkissanga.io\n",
      "\n",
      "- mangakakalot.com\n",
      "\n",
      "- olympusscanlation.com\n",
      "\n",
      "- syosetu.com\n",
      "\n",
      "- affinity.net\n",
      "For each URL above, provide at least two phishing-like variations in a list format. Only print the variations, one per line, without including the original URLs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_prompt(legit_urls):\n",
    "    prompt = \"Below is a list of legitimate URLs. Generate phishing-like variations for each URL that look visually similar but contain small differences commonly used in phishing attacks. Make sure each variation could be mistaken for the original.\\n\"\n",
    "    for url in legit_urls:\n",
    "        prompt += f\"\\n- {url}\\n\"\n",
    "    prompt += \"For each URL above, provide at least two phishing-like variations in a list format. Only print the variations, one per line, without including the original URLs.\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Crear las 4 listas, cada una con un cuarto del DataFrame\n",
    "lista = df_urls_legit['Domain'].to_list()\n",
    "prompt = generate_prompt(lista)\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='- g00gle.com  \\n- goggle.com  \\n\\n- y0utube.com  \\n- youube.com  \\n\\n- faceb00k.com  \\n- facebook.co  \\n\\n- pornhub.con  \\n- pornhub.com.co  \\n\\n- xvideoes.com  \\n- xvideos.site  \\n\\n- tw1tter.com  \\n- twitter.co  \\n\\n- w1kipedia.org  \\n- wikipedia.com  \\n\\n- redd1t.com  \\n- reditt.com  \\n\\n- instagran.com  \\n- instagram.co  \\n\\n- asura.gg.org  \\n- asura-g.g  \\n\\n- chapmanganato.co  \\n- chapmangnato.com  \\n\\n- xnnx.com  \\n- xnxx.xyz  \\n\\n- l3ctortmo.com  \\n- lect0rtmo.com  \\n\\n- yah00.com  \\n- yahoo.co  \\n\\n- spankbng.com  \\n- spankbang.co  \\n\\n- amazon.co  \\n- amaz0n.com  \\n\\n- xhamster18.com  \\n- xhamster.net  \\n\\n- weater.com  \\n- weather.co  \\n\\n- fan-dom.com  \\n- fandom.xyz  \\n\\n- yand3x.ru  \\n- yandex.ru.co  \\n\\n- nhent.ai  \\n- nhentai.org  \\n\\n- tik-tok.com  \\n- tiktok.co  \\n\\n- exdynsrv.io  \\n- exdynsrve.com  \\n\\n- yah00o.com.jp  \\n- yahoo.om.jp  \\n\\n- m4nganato.com  \\n- manganato.net  \\n\\n- duckduckgo.co  \\n- duckdocgo.com  \\n\\n- xhamster18.desi.com  \\n- xhamser18.desi  \\n\\n- animeflv.co  \\n- animeflv.net.uk  \\n\\n- searh-hub.com  \\n- searchhub.com  \\n\\n- live-door.jp  \\n- livedoor.io  \\n\\n- twitch-tv.com  \\n- twitch.tv.co  \\n\\n- blogsp0t.com  \\n- blogsp0t.org  \\n\\n- b1ng.com  \\n- bing.co  \\n\\n- dood.lu  \\n- dood.re.co  \\n\\n- archiveofourown.co  \\n- archiveofourown.org.uk  \\n\\n- rule34.site  \\n- rule34.xxx.net  \\n\\n- whasapp.com  \\n- whatsap.com  \\n\\n- liveonline.com  \\n- live.com.co  \\n\\n- bitll.y  \\n- bit.ly.co  \\n\\n- vk.ru  \\n- vk.com.co  \\n\\n- mangabuddy.co  \\n- mangabuddy.net  \\n\\n- youtu.be.co  \\n- y0utu.be  \\n\\n- linkedn.com  \\n- linkedin.co  \\n\\n- quora.xyz  \\n- qu0ra.com  \\n\\n- adsmloco.com  \\n- adsmollco.com  \\n\\n- pixiv.org  \\n- pixiv.nett  \\n\\n- chaturbate.co  \\n- chaturbat.com  \\n\\n- microsoft.org  \\n- micr0soft.com  \\n\\n- netflx.com  \\n- neflix.co  \\n\\n- h1toi.la  \\n- hitoi.laa  \\n\\n- tdot.me  \\n- t.me.co  \\n\\n- blog.jp.org  \\n- blog.jp.co  \\n\\n- viroanime.com  \\n- voiran.me  \\n\\n- office.co  \\n- office3.com  \\n\\n- navert.com  \\n- naver.ru  \\n\\n- micros0ftonline.com  \\n- microsoftonlne.com  \\n\\n- asurascans.co  \\n- asurascans.net  \\n\\n- imdb.co  \\n- imbd.com  \\n\\n- onlyfans.co  \\n- onlyfanz.com  \\n\\n- chapmanganelo.xyz  \\n- chapmanganelo.net  \\n\\n- komikcast.co  \\n- komikcast.me  \\n\\n- ts-tracker.info  \\n- ts-tracker.xyz  \\n\\n- cn.com  \\n- cnn2.com  \\n\\n- google-vid.com  \\n- googlevideo.co  \\n\\n- onelink.co  \\n- onelink.me.uk  \\n\\n- walmrt.com  \\n- walmat.com  \\n\\n- gogoanime.ai  \\n- gogo-anime.com  \\n\\n- reaper-scans.com  \\n- reaperscans.net  \\n\\n- paypall.com  \\n- paypal.co  \\n\\n- toonily.net  \\n- toonily.co  \\n\\n- hanie.tv.co  \\n- haniek.com  \\n\\n- aliexpres.com  \\n- aliexpres.org  \\n\\n- lectormanga.co  \\n- lectormanga.org  \\n\\n- marc-a.com  \\n- marca.co.uk  \\n\\n- mediafire.co  \\n- mediafie.com  \\n\\n- mangago.io  \\n- mangago.me.uk  \\n\\n- livedoor.net  \\n- livedoor.com.co  \\n\\n- webtoon.co  \\n- webtoons.xyz  \\n\\n- animxplay.io  \\n- animixplay.xyz  \\n\\n- bbc.co  \\n- bbc.uk  \\n\\n- pinterest.co  \\n- pinterest.com  \\n\\n- zoo.co.us  \\n- z0o.us  \\n\\n- mega.org.nz  \\n- mega.nz.co  \\n\\n- nytimes.co  \\n- nytim.es  \\n\\n- app-link.co  \\n- app.link.me  \\n\\n- mailru.com  \\n- mail.rus  \\n\\n- ebay.co  \\n- ebay.org  \\n\\n- twimg.com.co  \\n- twitimg.com  \\n\\n- disco1rd.com  \\n- discord.co  \\n\\n- git-hub.com  \\n- github.co  \\n\\n- nightskycluub.com  \\n- night-skyclub.com  \\n\\n- bbccom.co.uk  \\n- bbc.co.uk.com  \\n\\n- spo0tify.com  \\n- spotify.app  \\n\\n- apple.co  \\n- app1e.com  \\n\\n- sxyprn.con  \\n- sxyprn.com.co  \\n\\n- firstkissanga.io  \\n- 1stkissanga.co  \\n\\n- mangakakalot.net  \\n- mangakakalot.co  \\n\\n- olympusscan.net  \\n- olympusscanlation.com  \\n\\n- sy0sutu.com  \\n- syosetu.co  \\n\\n- affinity.co  \\n- affin1ty.net  ', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in cybersecurity and phishing detection.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_and_clean_domains(variations):\n",
    "    cleaned_domains = []\n",
    "    for url in variations:\n",
    "        url = url.strip()  # Remove extra spaces\n",
    "        if url:\n",
    "            # Find domain pattern, ignoring any prefix like \"- \" or similar\n",
    "            match = re.search(r'(\\b[\\w-]+\\.\\w+\\b)', url)\n",
    "            if match:\n",
    "                cleaned_domains.append(match.group(1))\n",
    "    return cleaned_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Domain  Phishing  Min_Levenshtein_Distance\n",
      "0    google.com         0                       NaN\n",
      "1   youtube.com         0                       NaN\n",
      "2  facebook.com         0                       NaN\n",
      "3   pornhub.com         0                       NaN\n",
      "4   xvideos.com         0                       NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_18076\\2573028206.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_legitm_url['Phishing'] = 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "\n",
    "# Suponiendo que ya tienes los DataFrames df_phishing y df_legitm_url\n",
    "variations1 = completion.choices[0].message.content.split(\"\\n\")\n",
    "phishing_urls1 = extract_and_clean_domains(variations1)\n",
    "\n",
    "df_phishing = pd.DataFrame({'Domain': phishing_urls1, 'Phishing': 1})\n",
    "\n",
    "df_legitm_url = df_urls_legit[['Domain']]\n",
    "df_legitm_url['Phishing'] = 0\n",
    "\n",
    "# Calcular la distancia de Levenshtein y obtener la menor distancia\n",
    "def get_min_levenshtein_distance(domain, legit_domains):\n",
    "    distances = [Levenshtein.distance(domain, legit_domain) for legit_domain in legit_domains]\n",
    "    min_distance = min(distances)\n",
    "    if min_distance == 0.0:\n",
    "        min_distance = 0.1\n",
    "    return min_distance\n",
    "\n",
    "legit_domains = df_legitm_url['Domain'].tolist()\n",
    "df_phishing['Min_Levenshtein_Distance'] = df_phishing['Domain'].apply(lambda x: get_min_levenshtein_distance(x, legit_domains))\n",
    "\n",
    "\n",
    "df_final = pd.concat([df_legitm_url, df_phishing], ignore_index=True)\n",
    "\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Domain  Phishing  Min_Levenshtein_Distance\n",
      "295  olympusscanlation.com         1                       0.1\n",
      "296            sy0sutu.com         1                       2.0\n",
      "297             syosetu.co         1                       1.0\n",
      "298            affinity.co         1                       3.0\n",
      "299           affin1ty.net         1                       1.0\n"
     ]
    }
   ],
   "source": [
    "print(df_final.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Domain  Phishing  Min_Levenshtein_Distance\n",
      "0          blog.jp         1                       0.1\n",
      "1       nytimes.co         1                       1.0\n",
      "2  xhamster18.desi         1                       0.1\n",
      "3         asura.gg         0                       0.0\n",
      "4   gogo-anime.com         1                       4.0\n"
     ]
    }
   ],
   "source": [
    "df_final[\"Min_Levenshtein_Distance\"] = df_final[\"Min_Levenshtein_Distance\"].fillna(0)\n",
    "# Mezclar las filas aleatoriamente\n",
    "df_final1 = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(df_final1.head())\n",
    "df_final1.to_csv('../datasets/Similarity_websites.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin_Levenshtein_Distance\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhishing\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Dividir los datos en conjuntos de entrenamiento y prueba\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "df = pd.read_csv('../datasets/Similarity_websites.csv')\n",
    "\n",
    "#\n",
    "\n",
    "# Separar las características y la etiqueta\n",
    "X = df[[\"Min_Levenshtein_Distance\"]]\n",
    "y = df[\"Phishing\"]\n",
    "print(x)\n",
    "print(y)\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de Random Forest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir y evaluar el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Ejemplo de predicción para un dominio con Min_Levenshtein_Distance = 2\n",
    "example_distance = [[5]]\n",
    "prediction = model.predict(example_distance)\n",
    "print(\"Prediction for Min_Levenshtein_Distance=2:\", \"Phishing\" if prediction[0] == 1 else \"Legit\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
