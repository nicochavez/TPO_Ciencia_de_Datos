{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Domain               100 non-null    object \n",
      " 1   Visits               100 non-null    float64\n",
      " 2   Desktop Share        100 non-null    float64\n",
      " 3   Mobile Share         100 non-null    float64\n",
      " 4   MoM                  98 non-null     float64\n",
      " 5   YoY                  94 non-null     float64\n",
      " 6   Main Traffic Source  100 non-null    object \n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 5.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/DataSet_WebPhishing_Final.csv')\n",
    "df_urls_legit = pd.read_csv('../datasets/top_websites.csv', delimiter=';')\n",
    "print(df_urls_legit.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_levenshtein_distance(url, legit_urls):\n",
    "    distances = [Levenshtein.distance(url, legit_url) for legit_url in legit_urls]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variantes de phishing generadas:\n",
      "['y0utube.c0m', 'youtub3.com', 'youtube.comy', 'youtube.co', 'faceb00k.c0m', 'fac3book.com', 'f4cebook.com', 'facebook.comv', 'facebook.co', 'g00gle.c0m', 'googl3.com', 'google.coml', 'google.co']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generar_variaciones(url):\n",
    "    variaciones = []\n",
    "    # Reemplazo de caracteres comunes\n",
    "    reemplazos = {'o': '0', 'i': '1', 'e': '3', 'a': '4', 's': '5'}\n",
    "    \n",
    "    # Generar una variación reemplazando letras\n",
    "    for original, reemplazo in reemplazos.items():\n",
    "        if original in url:\n",
    "            variaciones.append(url.replace(original, reemplazo))\n",
    "    \n",
    "    # Agregar o eliminar letras\n",
    "    variaciones.append(url + random.choice(\"abcdefghijklmnopqrstuvwxyz\"))\n",
    "    variaciones.append(url[:-1])  # Eliminar el último caracter\n",
    "    \n",
    "    return variaciones\n",
    "\n",
    "# Ejemplo con URLs legítimas\n",
    "legit_urls = [\"youtube.com\", \"facebook.com\", \"google.com\"]\n",
    "\n",
    "phishing_urls = []\n",
    "\n",
    "for url in legit_urls:\n",
    "    phishing_urls.extend(generar_variaciones(url))\n",
    "\n",
    "print(\"Variantes de phishing generadas:\")\n",
    "print(phishing_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a list of legitimate URLs. Generate phishing-like variations for each URL that look visually similar but contain small differences commonly used in phishing attacks. Make sure each variation could be mistaken for the original.\n",
      "\n",
      "- google.com\n",
      "\n",
      "- youtube.com\n",
      "\n",
      "- facebook.com\n",
      "\n",
      "- pornhub.com\n",
      "\n",
      "- xvideos.com\n",
      "\n",
      "- twitter.com\n",
      "\n",
      "- wikipedia.org\n",
      "\n",
      "- reddit.com\n",
      "\n",
      "- instagram.com\n",
      "\n",
      "- asura.gg\n",
      "\n",
      "- chapmanganato.com\n",
      "\n",
      "- xnxx.com\n",
      "\n",
      "- lectortmo.com\n",
      "\n",
      "- yahoo.com\n",
      "\n",
      "- spankbang.com\n",
      "\n",
      "- amazon.com\n",
      "\n",
      "- xhamster.com\n",
      "\n",
      "- weather.com\n",
      "\n",
      "- fandom.com\n",
      "\n",
      "- yandex.ru\n",
      "\n",
      "- nhentai.net\n",
      "\n",
      "- tiktok.com\n",
      "\n",
      "- exdynsrv.com\n",
      "\n",
      "- yahoo.com.jp\n",
      "\n",
      "- manganato.com\n",
      "\n",
      "- duckduckgo.com\n",
      "\n",
      "- xhamster18.desi\n",
      "\n",
      "- animeflv.net\n",
      "\n",
      "- search-hub.com\n",
      "\n",
      "- livedoor.jp\n",
      "\n",
      "- twitch.tv\n",
      "\n",
      "- blogspot.com\n",
      "\n",
      "- bing.com\n",
      "\n",
      "- dood.re\n",
      "\n",
      "- archiveofourown.org\n",
      "\n",
      "- rule34.xxx\n",
      "\n",
      "- whatsapp.com\n",
      "\n",
      "- live.com\n",
      "\n",
      "- bit.ly\n",
      "\n",
      "- vk.com\n",
      "\n",
      "- mangabuddy.com\n",
      "\n",
      "- youtu.be\n",
      "\n",
      "- linkedin.com\n",
      "\n",
      "- quora.com\n",
      "\n",
      "- adsmoloco.com\n",
      "\n",
      "- pixiv.net\n",
      "\n",
      "- chaturbate.com\n",
      "\n",
      "- microsoft.com\n",
      "\n",
      "- netflix.com\n",
      "\n",
      "- hitoi.la\n",
      "\n",
      "- t.me\n",
      "\n",
      "- blog.jp\n",
      "\n",
      "- voiranime.com\n",
      "\n",
      "- office.com\n",
      "\n",
      "- naver.com\n",
      "\n",
      "- microsoftonline.com\n",
      "\n",
      "- asurascans.com\n",
      "\n",
      "- imdb.com\n",
      "\n",
      "- onlyfans.com\n",
      "\n",
      "- chapmanganelo.com\n",
      "\n",
      "- komikcast.site\n",
      "\n",
      "- ts-tracker.me\n",
      "\n",
      "- cnn.com\n",
      "\n",
      "- googlevideo.com\n",
      "\n",
      "- onelink.me\n",
      "\n",
      "- walmart.com\n",
      "\n",
      "- gogoanime.ar\n",
      "\n",
      "- reaperscans.com\n",
      "\n",
      "- paypal.com\n",
      "\n",
      "- toonily.com\n",
      "\n",
      "- hanie.tv\n",
      "\n",
      "- aliexpress.com\n",
      "\n",
      "- lectormanga.com\n",
      "\n",
      "- marca.com\n",
      "\n",
      "- mediafire.com\n",
      "\n",
      "- mangago.me\n",
      "\n",
      "- livedoor.com\n",
      "\n",
      "- webtoon.xyz\n",
      "\n",
      "- animixplay.to\n",
      "\n",
      "- bbc.com\n",
      "\n",
      "- pinterest.com\n",
      "\n",
      "- zoo.us\n",
      "\n",
      "- mega.nz\n",
      "\n",
      "- nytimes.com\n",
      "\n",
      "- app.link\n",
      "\n",
      "- mail.ru\n",
      "\n",
      "- ebay.com\n",
      "\n",
      "- twimg.com\n",
      "\n",
      "- discord.com\n",
      "\n",
      "- github.com\n",
      "\n",
      "- nightskyclub.com\n",
      "\n",
      "- bbc.co.uk\n",
      "\n",
      "- spotify.com\n",
      "\n",
      "- apple.com\n",
      "\n",
      "- sxyprn.com\n",
      "\n",
      "- 1stkissanga.io\n",
      "\n",
      "- mangakakalot.com\n",
      "\n",
      "- olympusscanlation.com\n",
      "\n",
      "- syosetu.com\n",
      "\n",
      "- affinity.net\n",
      "For each URL above, provide at least twenty phishing-like variations in a list format. Only print the variations, one per line, without including the original URLs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_prompt(legit_urls):\n",
    "    prompt = \"Below is a list of legitimate URLs. Generate phishing-like variations for each URL that look visually similar but contain small differences commonly used in phishing attacks. Make sure each variation could be mistaken for the original.\\n\"\n",
    "    for url in legit_urls:\n",
    "        prompt += f\"\\n- {url}\\n\"\n",
    "    prompt += \"For each URL above, provide at least twenty phishing-like variations in a list format. Only print the variations, one per line, without including the original URLs.\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Crear las 4 listas, cada una con un cuarto del DataFrame\n",
    "lista = df_urls_legit['Domain'].to_list()\n",
    "prompt = generate_prompt(lista)\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in cybersecurity and phishing detection.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_and_clean_domains(variations):\n",
    "    cleaned_domains = []\n",
    "    for url in variations:\n",
    "        url = url.strip()  # Remove extra spaces\n",
    "        if url:\n",
    "            # Find domain pattern, ignoring any prefix like \"- \" or similar\n",
    "            match = re.search(r'(\\b[\\w-]+\\.\\w+\\b)', url)\n",
    "            if match:\n",
    "                cleaned_domains.append(match.group(1))\n",
    "    return cleaned_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Domain  Phishing\n",
      "0    google.com         0\n",
      "1   youtube.com         0\n",
      "2  facebook.com         0\n",
      "3   pornhub.com         0\n",
      "4   xvideos.com         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_50676\\1154179635.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_legitm_url_1['Phishing'] = 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "\n",
    "# Suponiendo que ya tienes los DataFrames df_phishing y df_legitm_url\n",
    "variations1 = completion.choices[0].message.content.split(\"\\n\")\n",
    "phishing_urls1 = extract_and_clean_domains(variations1)\n",
    "\n",
    "df_phishing = pd.DataFrame({'Domain': phishing_urls1, 'Phishing': 1})\n",
    "\n",
    "df_phishing.to_csv('../datasets/phishing_variations.csv', index=False)\n",
    "\n",
    "df_legitm_url_1 = df_urls_legit[['Domain']]\n",
    "df_legitm_url_1['Phishing'] = 0\n",
    "\n",
    "df_legitm_url_2 = pd.read_csv('../datasets/topsites_etv.csv', usecols=['Domain'])\n",
    "df_legitm_url_2['Phishing'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Calcular la distancia de Levenshtein y obtener la menor distancia\n",
    "# def get_min_levenshtein_distance(domain, legit_domains):\n",
    "#     distances = [Levenshtein.distance(domain, legit_domain) for legit_domain in legit_domains]\n",
    "#     min_distance = min(distances)\n",
    "#     if min_distance == 0.0:\n",
    "#         min_distance = 0.1\n",
    "#     return min_distance\n",
    "\n",
    "# legit_domains = df_legitm_url['Domain'].tolist()\n",
    "# df_phishing['Min_Levenshtein_Distance'] = df_phishing['Domain'].apply(lambda x: get_min_levenshtein_distance(x, legit_domains))\n",
    "\n",
    "\n",
    "df_final = pd.concat([df_legitm_url_1, df_legitm_url_2, df_phishing], ignore_index=True)\n",
    "\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mezclar las filas del DataFrame df_final\n",
    "df_final_shuffled = df_final.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Guardar el DataFrame mezclado en un archivo CSV\n",
    "df_final_shuffled.to_csv('../datasets/DataSet_WebPhishing_embedding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Domain  Phishing  Min_Levenshtein_Distance\n",
      "295  olympusscanlation.com         1                       0.1\n",
      "296            sy0sutu.com         1                       2.0\n",
      "297             syosetu.co         1                       1.0\n",
      "298            affinity.co         1                       3.0\n",
      "299           affin1ty.net         1                       1.0\n"
     ]
    }
   ],
   "source": [
    "print(df_final.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_final[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin_Levenshtein_Distance\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_final\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin_Levenshtein_Distance\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(random\u001b[38;5;241m.\u001b[39mranint(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Mezclar las filas aleatoriamente\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_final1 \u001b[38;5;241m=\u001b[39m df_final\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_final' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "df_final[\"Min_Levenshtein_Distance\"] = df_final[\"Min_Levenshtein_Distance\"].fillna(random.ranint(3, 8))\n",
    "# Mezclar las filas aleatoriamente\n",
    "df_final1 = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(df_final1.head())\n",
    "df_final1.to_csv('../datasets/Similarity_websites.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin_Levenshtein_Distance\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhishing\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Dividir los datos en conjuntos de entrenamiento y prueba\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "df = pd.read_csv('../datasets/Similarity_websites.csv')\n",
    "\n",
    "#\n",
    "\n",
    "# Separar las características y la etiqueta\n",
    "X = df[[\"Min_Levenshtein_Distance\"]]\n",
    "y = df[\"Phishing\"]\n",
    "print(x)\n",
    "print(y)\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de Random Forest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir y evaluar el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Ejemplo de predicción para un dominio con Min_Levenshtein_Distance = 2\n",
    "example_distance = [[5]]\n",
    "prediction = model.predict(example_distance)\n",
    "print(\"Prediction for Min_Levenshtein_Distance=2:\", \"Phishing\" if prediction[0] == 1 else \"Legit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Domain  Phishing\n",
      "0   kotobank.jp         0\n",
      "1      hgtv.com         0\n",
      "2  pintrist.com         1\n",
      "3  searhhub.com         1\n",
      "4   amaazon.com         1\n",
      "Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv('../datasets/DataSet_WebPhishing_embedding.csv')\n",
    "\n",
    "df = data[['Domain', 'Phishing']]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Paso 2: Crear embeddings usando TF-IDF basado en caracteres\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))  # 2 a 4-gramas de caracteres\n",
    "X = vectorizer.fit_transform(df[\"Domain\"])\n",
    "y = df[\"Phishing\"]\n",
    "\n",
    "# Paso 3: Separar datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 4: Entrenar el modelo de clasificación\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Paso 5: Evaluar el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Min_Levenshtein_Distance=2: Phishing\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(vectorizer.transform([\"yotube.com\"]))\n",
    "print(\"Prediction for Min_Levenshtein_Distance=2:\", \"Phishing\" if prediction[0] == 1 else \"Legit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: yotube.com\n",
      "Mensaje: Probabilidad de phishing\n",
      "Probabilidad de phishing: 0.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paso 1: Cargar y preparar el dataset\n",
    "data = pd.read_csv('../datasets/DataSet_WebPhishing_embedding.csv')\n",
    "\n",
    "# Crear características con TF-IDF (basado en caracteres)\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "X = vectorizer.fit_transform(data['Domain'])\n",
    "y = data['Phishing']\n",
    "\n",
    "# Paso 2: Entrenar el modelo de Isolation Forest para detectar anomalías en URLs legítimas\n",
    "legit_X = X[y == 0]  # Solo URLs legítimas\n",
    "anomaly_detector = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_detector.fit(legit_X)  # Detectar anomalías en URLs legítimas\n",
    "\n",
    "# Paso 3: Entrenar el modelo de clasificación con Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Paso 4: Función para predecir si una URL es phishing, legítima o desconocida\n",
    "def predict_url(url):\n",
    "    # Vectorizar la URL\n",
    "    url_vector = vectorizer.transform([url])\n",
    "    \n",
    "    # Detección de anomalía con Isolation Forest\n",
    "    is_anomalous = anomaly_detector.predict(url_vector)  # -1 es anómalo, 1 es normal\n",
    "    \n",
    "    if is_anomalous == -1:\n",
    "        return {\"message\": \"URL desconocida o atípica\", \"probability\": None}  # URL muy diferente de las legítimas\n",
    "    \n",
    "    # Si la URL no es anómala, evaluamos con el clasificador Random Forest\n",
    "    proba = clf.predict_proba(url_vector)[:, 1]  # Obtener la probabilidad de ser phishing\n",
    "    return {\"message\": \"Probabilidad de phishing\", \"probability\": proba[0]}\n",
    "\n",
    "# Paso 5: Probar con una URL nueva\n",
    "new_url = \"yotube.com\"\n",
    "result = predict_url(new_url)\n",
    "\n",
    "print(f\"URL: {new_url}\")\n",
    "print(f\"Mensaje: {result['message']}\")\n",
    "print(f\"Probabilidad de phishing: {result['probability']:.2f}\" if result['probability'] is not None else \"No se pudo calcular la probabilidad\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
